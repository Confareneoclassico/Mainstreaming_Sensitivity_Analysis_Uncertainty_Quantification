﻿
# Sensitivity auditing

<!-- AUTHOR: Andrea Saltelli -->

**Andrea Saltelli**, [andrea.saltelli@uib.no](mailto:andrea.saltelli@uib.no)

## In brief

Sensitivity auditing includes and complements technical uncertainty and [sensitivity analysis](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470725184) – to ensure the quality and relevance of model based inference. An uncertainty analysis tells how uncertain the model-based inference is. A well run sensitivity analysis identifies which among the input assumptions and uncertain factors is more important in driving the uncertainty in the model-based prediction. Sensitivity auditing is an extension of sensitivity analysis which is necessary when the model is used in a context of policy. Here the motivations of the modelling and the associated interests and power relationships become part of the analysis – especially when competing evidence is being proposed.

## Questions addressed

What tools, practices, critical skills and processes of quality control are needed for models used at the science-governance interface to aid in the responsible development and use of models and indicators? How to judge the quality of models – including economic models – applied to the treatment of complex environmental issues? How to appraise the quality of a technical sensitivity analysis? How to test for hidden assumptions? How to identify amplification or underestimation of the known uncertainties? How to ensure model transparency? How to test that a model’s complexity is relevant to the issue and not a tool for obfuscation or displacement? How to check that the framing of an issue allows for alternative plausible and legitimate visions of what the problem is?

## Overall context

A diligent sensitivity auditing would require a toolbox composed of uncertainty analysis, global sensitivity analysis, sensitivity auditing proper and quantitative storytelling. In the figure, we put in relation the steps of sensitivity auditing and quantitative storytelling with [NUSAP](WebResources.ipynb) described elsewhere in this notebook, and with a recent checklist for model quality put forward by Jakeman et al. 2006. Both the work of these authors and ours reflect a growing concern for the quality of the modelling practice – especially in relation with the discipline-specific arrangement of the modelling practice, whereby ‘modelling’ in nowhere taught as an independent discipline [Saltelli, 2017](https://arxiv.org/abs/1712.06457).

![scheme](./Scheme.png)

## Uncertainty analysis

Uncertainty analysis focuses on quantifying the uncertainty in model output. <br/> Uncertainty analysis is what all modelling activities should start from. Instead of being an _a posteriori_ check of the modelling activity, a propagation of the uncertainties due to the uncertain input factors and assumptions should perhaps be the way a model is built [Saltelli, 2017](https://arxiv.org/abs/1712.06457). A good craftsman is naturally interested in monitoring the build-up of the uncertainty in the model-based inference/prediction as new processes are incorporated in the model, and by systematically running the model a set of times - as opposed to once - at the stage of model construction, this monitoring can be done orderly. Unfortunately what is seen is current practices is that - as computational resources available to modellers keep increasing - modellers naturally adapt by complexifying or just complicating their model so that the execution times remain 'important'.  

The consequence is that models are built whose relevance is unknown. A model whose uncertainty has not been tested could produce results which are close to the toss of a coin once one changes its input factors within plausible bounds. Scholars from various disciplines (hydrology, economics, PNS) have noted that a modeller might resort to ‘massaging’, e.g. arbitrarily reducing, the uncertainty in the input so as to avoid the situation where uncertainty in the output is too large.

## Sensitivity analysis

Sensitivity analysis is the study of the relative importance of different uncertain input factors in determining the uncertainty in the model output.  

A global quantitative sensitivity analysis (see [Saltelli et al., 2008](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470725184) for a popular textbook) is the ideal complement to uncertainty analysis. In this kind of analysis all factors are varied simultaneously and the space of the input factors is explored systematically. Doing a sensitivity analysis moving only one factor at a time is close to useless – and non-conservative - as it does not explore the space of the input factors [Saltelli and Annoni 2010](https://www.sciencedirect.com/science/article/pii/S1364815210001180). Varying one factor at a time even more than once corresponds to moving along mono-dimensional corridors which leave the space of the input factors largely unexplored. Additional these mono-dimensional trajectories of exploration do not activate synergy between factors (what statisticians call interactions) which can be critical instead in understanding model behaviour. Two factor may act synergistically to produce e.g. extreme values of the output of interest.

A global sensitivity analysis tells the modeller which input factor or assumptions drives the uncertainty, and which is instead totally uninfluential. As discussed above this information may be precious even at the model building stage, but a fortiori one might be interested in whether a model has undergone such a test before being used to make predictions.

Interactive notebooks on sensitivity analysis are found in the [last-year collection](https://github.com/lrhgit/uqsa_tutorials) as well as in the [dedicated section of this new series](https://github.com/pbstark/SA).

## Sensitivity auditing: a checklist

Scientific evidence presented for the analysis of – or in support to – a given policy may be conflicted and disputed. In upholding their peculiar knowledge claims, all sides in disputes may be guilty of inappropriate generalizations, hidden value judgements and misrepresentation of the other parties’ arguments. In these situations, a model-based assessment may be particularly vulnerable, e.g. to the choice of the model itself, to the institutional or industrial setting where the model was developed, and to the framing of the study.

According to the tenets of post normal science [(PNS)](https://en.wikipedia.org/wiki/Post-normal_science), these tensions should not be resolved by discarding science, but by investing more in the analysis of the quality of the process on which the evidence has been constructed. This can be achieved by a process of ‘extended participation’, in which the investigation and analysis are open not only to experts from different disciplines and forms of scholarship, but also to the active participation of relevant and legitimate stakeholders. PNS offers among its tools sensitivity auditing [Saltelli et al., 2013](https://www.inderscienceonline.com/doi/abs/10.1504/IJFIP.2013.058610?journalCode=ijfip); [Saltelli & Funtowicz, 2014](http://issues.org/30-2/andrea/), which could be used to gauge the reliability, relevance and legitimacy of model-based inferences.

Applying sensitivity auditing implies running through a checklist:

1. *Rule 1*: ‘Check against rhetorical use of mathematical modelling’; are results being over-interpreted?  Is the model being used ritually or rhetorically?  
2. *Rule 2*: ‘Adopt an “assumption hunting” attitude’; this would focus on unearthing possibly implicit assumptions.
3. *Rule 3*: ‘Detect pseudo-science’; this asks whether uncertainty has been downplayed, as discussed above, in order to present results in a more favourable light. This rule can be referred to as GIGO, from garbage in – garbage out, or as detect pseudo-science (Funtowicz and Ravetz, 1990).
4. *Rule 4*: ‘Find sensitive assumptions before these find you’; this is a reminder that before publishing results the analysis of sensitivity should be done and made accessible to researchers.
5. *Rule 5*: ‘Aim for transparency’. This rule echoes present debates on open data, and of the need for a third party to be able to replicate a given analysis, see e.g. the Peer Reviewers' Openness Initiative (Morey et al., 2016) intended to discipline authors into providing complete access to the materials used in the preparation of articles, or the [San Francisco declaration](http://www.ascb.org/dora/), as well as [J.P.A. Ioannidis’s paper on ‘How to Make More Published Research True’](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001747).
6. *Rule 6*: ‘Do the right sums’; the analysis should not solve the wrong problem – doing the right sums is more important than doing the sums right. This points to quantitative storytelling discussed below. In summary this rule is about asking whether the given quantification is not neglecting important alternatives ways to frame a given example.
7. *Rule 7*: ‘Focus the analysis on the key question answered by the model, exploring holistically the entire space of the assumptions’. This rule is a reminder of good sensitivity analysis practice to run sensitivity analysis globally. Additionally, the object of the analysis should not be the model per se, but the inference of policy being supported by the model. Fragility of volatility are in fact not attributes of the model as such, but of the model as used to answer a particular question. An important implication of this rule is that a model cannot be audited for sensitivity once and for all, but needs to be re-audited in the context of each specific application of the model.

This checklist may be seen to pose a burden on the analyst; however, when a scientific analysis is destined to inform an important policy process, it is reasonable to ask that methodological standards be set high.

## Quantitative storytelling

Quantitative storytelling (QST) expands on rule 6 of sensitivity auditing by asking the question of ‘what to do’ in order to avoid that an issue is framed unilaterally.  

QST assumes that in an interconnected society more frameworks and worldviews are legitimately upheld by different constituencies and social actors. QST looks critically to models used in evidence-based policy (EBP). These are often in the form of risk analyses or cost benefit analyses, and necessarily focus on a single framing of the issue under consideration. In the logic of QST, the deepening of the analysis corresponding to a single view of what the problem is runs the risk of distracting from what could be alternative readings.

Alternative frames [Ravetz, 1987](http://journals.sagepub.com/doi/abs/10.1177/107554708700900104); [Rayner, 2012](https://www.tandfonline.com/doi/abs/10.1080/03085147.2011.637335) may represent ‘uncomfortable knowledge’, which is removed from the policy discourse. Thus, extensive mathematical modelling in EBP to support a given policy may lead to a simplification of the available perceptions and generate - rather than resolve – controversies. The world ‘hypo-cognition’ has been used in the context of these instrumental use of frames [Lakoff et al., 2008](https://books.google.es/books?id=zbJ1oxHC9a0C); [Lakoff, 2010](https://www.tandfonline.com/doi/abs/10.1080/17524030903529749).

Under this critical viewpoint, mathematical models can be seen as a tool for ‘displacement’. Displacement occurs where a model becomes the end instead of the tool, e.g. when an institution chooses to monitor and manage the outcome of a model rather than what happens in reality [Rayner, 2012](https://www.tandfonline.com/doi/abs/10.1080/03085147.2011.637335). Once exposed, the strategic use of hypo-cognition erodes the trust in the involved actors and institutions.

QST suggests acknowledging ignorance, as to work out ‘clumsy solutions’ [Rayner, 2012](https://www.tandfonline.com/doi/abs/10.1080/03085147.2011.637335), which may accommodate unshared epistemological or ethical principles. This is in turn close to the PNS tenet of ‘working deliberatively within imperfections’ [van der Sluijs and Petersen, 2008](http://iopscience.iop.org/article/10.1088/1748-9326/3/2/024008), and to the exigence for a ‘rediscovery of ignorance’ (see preface to Pereira and Funtowicz, 2015).

QST also calls attention to the power relationships at play in the use of evidence. [Saltelli and Giampietro 2014](https://www.sciencedirect.com/science/article/pii/S1470160X14000387) suggest that our present approach to evidence-based policy, even in the more nuanced formulation of evidence-informed policy [Gluckman, 2014](https://www.nature.com/news/policy-the-art-of-science-advice-to-government-1.14838) – needs our urgent attention. Unavoidable asymmetries are generated by the fact that stronger players have access to better evidence, and can use it strategically [Boden and Epstein, 2006](https://www.tandfonline.com/doi/full/10.1080/14767720600752619?src=recsys); [Strassheim and Kettunen, 2014](http://www.ingentaconnect.com/search/article?option2=author&value2=Strassheim&freetype=unlimited&sortDescending=true&sortField=default&pageSize=10&index=5)). The decline of pollinators challenge [Insectageddon, Mombiot, 2017](https://www.theguardian.com/commentisfree/2017/oct/20/insectageddon-farming-catastrophe-climate-breakdown-insect-populations) show that interest groups have more scope to capture regulators than the average citizen ad consumer.

QST encourages an effort in the pre-analytic, pre-quantitative phase of the analysis to map a socially robust (i.e. inclusive of the interest of different stakeholders) universe of possible frames.

Obviously, the medicine for a diseased evidence-based policy is not a prejudice- or superstition-based policy, but a more democratic and dialogic access to the provision of evidence – even in terms of agenda setting. For this a new institutional setting is needed.

QST does not eschew the use quantitative tools altogether. It suggests instead to explore quantitatively multiple narratives, avoiding spurious accuracy and focusing on some salient features of the selected stories. Rather than attempting to amass evidence in support to a given reading or policy, or to optimise it with modelling, QST operates ‘via negativa’, i.e. it tries to test whether the said framing runs afoul of a quantitative or qualitative analytical check. Here QST borrows from system ecology and attempts to refute the frames if these violate constraints of (Giampietro et al., 2014):

1. feasibility (can we afford a given policy in terms of external constraints, e.g. existing biophysical resources?);
2. viability (can we afford it in the context of our internal constraints, governance, socioeconomic and technological arrangements?); and
3. desirability (will the relevant constituency accept it?).

While we shall see in the following section application of QST, perhaps the best application of the concept of QST is an old study of GMO-related perceptions [Marris, 2001](http://csec.lancs.ac.uk/archive/pabe/docs/pabe_finalreport.pdf) which has lost very little of its actuality since the ongoing GMO and pesticide debate.
By direct interview and measurements of stakeholders’ expectation and worldviews, Marris and co-authors showed that the prevailing narrative of the reaction to GMO as a ‘food scare’ – i.e. as an issue of safety to consume GMO food – did not show up among the concerns raised by the interviewed citizens, which worried instead about who would benefit from these technologies, why were they introduced in the first place, and whether existing regulatory authorities would be up to the task of resisting regulatory capture from powerful industrial incumbents.

## What results can be expected from the sensitivity auditing toolbox

The toolbox provides Improved tools, practices, and the opportunity for developing critical skills and processes of quality control for responsible use of models and indicators, e.g. by allowing a reading of models and indicators so as to test them against spurious accuracy and unilateral framing of the analysis. This fosters parsimonious and transparent model representations, and allows testing and comparing policy options in terms of their feasibility, viability, and desirability while allowing for the uncertainties in the measures taken to represent these dimensions.  

In practice it is not always true that uncertainty prevents the taking of decision, and that uncertainty needs to be equated with low quality. An analysis with a careful appreciation of the uncertainties can still allow option to be compared effectively, and be hence considered as an analysis of good quality.

## An application of sensitivity auditing: Mathematical modelling for the costing of action versus inaction in climate studies

### Climate cost: Introduction to the case

Most scientists are of the opinion that humanity is conducting a large-scale geophysical experiment with the planet by increasing the concentration of greenhouse gases.
The thesis is essentially correct, and it has been presented by large sectors of the scientific communities as a scientific consensus concerning the policy solution of phasing out fossil fuels. In policy terms, the thesis has been successful, and has largely driven humanity’s policy agenda, as evidenced by the Paris Agreement of 2015.
Still, reasonable minds can differ on the urgency or the feasibility of the strategy for mitigating global warming.  For discussion of feasibility see e.g. [Vaclav Smil 2014](https://www.scientificamerican.com/article/a-global-transition-to-renewable-energy-will-take-many-decades/).  This is one reason why observers on both sides of the “act now!” versus “wait and see” camps cannot agree on how to tame the doubt hounding both climate research and effective responses to the challenge [Revkin, 2017](https://www.propublica.org/article/climate-change-uncertainties-bret-stephens-column); [Stephens, 2017](https://www.nytimes.com/2017/04/28/opinion/climate-of-complete-certainty.html). This is the issue where the ‘Uncertainty Monster’, to use a definition of [Jeroen van der Sluijs 2005](https://www.ncbi.nlm.nih.gov/pubmed/16304939), dominates.
It is possibly for this reason that, in order to win the battle of hearts and minds on the issue, economists have entered the fray bringing with them possibly well-intentioned instruments of quantification to show how convenient – and hence appropriate – urgent action is. Nicholas Stern’s book [Stern, 2015](https://mitpress.mit.edu/books/why-are-we-waiting) on the subject is aptly titled “Why are we waiting? The logic, urgency, and promise of tackling climate change”.
However, existing quantification of the cost of mitigating or adapting to climate change defy scientific rigour, and at times verge on the hilarious, were it not for the fact that considerable resources are invested in these exercises. In this application of sensitivity auditing we discuss why these quantifications should be regarded as highly speculative and not be taken as a justification for policy.

### Climate cost: Analysis/application of the tool

A first look at the uncertainty of the costing of climate change can be achieved using exclusively technical uncertainty and sensitivity analysis. This was attempted [Saltelli and d’Hombres, 2010](https://www.sciencedirect.com/science/article/pii/S0959378009001228) taking as a point of departure the so called [“Stern Review”](http://webarchive.nationalarchives.gov.uk/20100407172811/http://www.hm-treasury.gov.uk/stern_review_report.htm), a review of the economics of climate change commissioned by the UK government. The work by Saltelli and d’Hombres was a reaction to a dispute between the same Stern and William Nordhaus, an expert in cost benefit analyses of climate, who claimed [Nordhaus, 2007](http://science.sciencemag.org/content/317/5835/201) to reach conclusions opposite to those of Stern by just adopting different discount factors. These are technical coefficients to account for the fact that future gains and losses should be discounted with regard to present ones (simplifying). In order to react to the criticism Stern extended his review with an annex to show how an uncertainty analysis would vindicate his position that the damage from climate change is large, thus justifying urgent action [Stern and Taylor, 2007](http://science.sciencemag.org/content/sci/317/5835/203.full.pdf?ck=nck).  
The technical re-analysis of Stern’s work by Saltelli and d’Hombres has shown that if all uncertainties are properly accounted for – taking Stern’s own uncertainty distributions – then the uncertainty in the cost of climate damage is so broad as to preclude any conclusion as to the urgency to act at the present time to counteract it, see Figure below.

The figure is from Saltelli and d'Hombres 2010. The abscissa represents the damage expressed in loss of GDP points at year 2100 and the ordinate its probability distribution. Value of the abscissa close to one could be used to justify a wait and see attitude while values close e.g. to five or higher could be used to call for urgent action. As it stands the plot – generated using Stern own data – seem to suggest that we do not know.
Evidently the same critique applies to William Nordhaus who wishes to reach the opposite conclusion based on the same class of models.
We were not the first to ‘discover’ the fragility of these analysis. Writing in 1994 Funtowicz and Ravetz did a similar analysis using NUSAP (see section 5.1) to deconstruct a cost estimate of the same Willem Nordhaus aimed – guess - to say that climate change would have had a modest impact.

![Stern](./Stern.png)

As mentioned above the discussion on the economics of climate change was reinvigorated by [Stern’s book](https://mitpress.mit.edu/books/why-are-we-waiting) and related article on Nature entitled “Economics: Current climate models are grossly misleading” – meaning by this that models in use were under-representing the risk. For this Stern called for better models to be used: dynamic stochastic computable general equilibrium (DSGE) models, and agent-based models (ABMs).
It may be recalled that DSGE models were in the eye of the storm for not having predicted the latest recession [Mirowski, 2013](https://www.versobooks.com/books/1613-never-let-a-serious-crisis-go-to-waste) so it has been somewhat surprising to see that they could illuminate us on the effects of climate change. One does not understand how both DSGE and agent-based models will cope with the ‘uncertainty monster’ better than the previous generation of models. The machinery of costing climate change reached the extreme when a team at Berkeley produced the “American Climate Prospectus: Economic Risks in the United States” [Rhodium Group, 2014](http://science.sciencemag.org/content/356/6345/1362)
This was a study commissioned by the Risky Business Project, a non-profit group chaired by former New York mayor Michael Bloomberg, former U.S. Treasury Secretary Henry Paulson, and environmental philanthropist Tom Steyer. The report produced forecasts—at the level of individual counties in the U.S.—of energy costs and demand, labour supply, mortality, violent crime rates [sic], and real estate property prices up to the year 2100. As noted in [Saltelli et al. 2015](http://issues.org/31-3/climate-models-as-economic-guides-scientific-challenge-or-quixotic-quest/) the report presented the amount of computer power and data generated as evidence of the scientific legitimacy of the enterprise. Predicting crime rate at county level in 2100 can indeed be many things, but hardly a scientific activity.

### Climate cost: Key outcomes and lessons learnt

In a correspondence published in Nature a group of six authors [Saltelli et al., 2016](https://www.nature.com/articles/532177a) noted that “Models that predict higher costs of climate change might make political intervention more palatable. But prescribing models that generate more precisely quantified estimates of a desired output is a political programme, not a scientific one. Responsible research requires responsible quantification and responsible acknowledgement of uncertainty”. While the position expressed in this letter might not be shared by all, and while understandably economists are part of the climatic debate, the experience we gained in these analyses suggest extreme caution in the use of economic argument of this nature to orient the debate into any possible direction.

Climate change is a key topic in the public discourse and models for the costing of action versus inaction in addressing climate change are seen by many as a key ingredient of a policy action and advocacy for urgent action [Stern, 2015](https://mitpress.mit.edu/books/why-are-we-waiting). These models are particularly vulnerable to deconstruction giving the difficulty to predict meaningfully both socioeconomic and ecological developments decades away from the present. While these studies may be seen as academically legitimate, and even useful for policy simulation, their use for policy formulation should be regarded as dubious.

## A second test case for sensitivity auditing: The Ecological footprint of the Ecological Footprint Network

One of the most famous models used in sustainability science is the Ecological Footprint (EF) developed by the Global Footprint Network ([GFN](https://www.footprintnetwork.org/)). The mathematical protocol developed by GFN aims to assess man’s impact on the planet and wishes to achieve this by aggregating across scales and compartments. Thus, the result of the analysis can be communicated as an overall measure of man’s impact on the planet, such as the ‘Earth overshoot day’. Stating that ‘August 2 is [Earth Overshoot Day](www.overshootday.org) 2017’, and that in less than 8 months, humanity exhausts Earth's budget for the year is a clear answer to the question of man’s overall impact on the planet. This goes with saying that man needs 1.6 planets instead of the one available.

The success of the Ecological Footprint is extraordinary. It is taken up by a myriad of countries and well-intended organisations, such as the World Wildlife Fund, the United Nations Environment Program, the United Nations Development Program, the International Union for Conservation of Nature, and the Convention on Biological Diversity, and others [Giampietro and Saltelli, 2014](https://www.sciencedirect.com/science/article/pii/S1470160X14000387). The EF is also very successful with the media.  
Unfortunately, EF’s numbers – clear and precise as they are - are a misleading – and in a sense reassuring - abstraction. Depending on what dimension of impact of man on the planet is looked at these number could change dramatically, and mostly for the worse. To stay with the example of August 2, 2017 as earth overshot day, this date could very well be January 2017, or a year in the past if the irreversible impacts of man on earth were to be considered. This has been noted by several practitioners as discussed next section. This test case relies on system ecology, and was the subject of a series of papers on the journal Ecological Indicators.

1. A [first paper](https://www.sciencedirect.com/science/article/pii/S1470160X14000387) of critique by Saltelli and Giampietro
2. A [reply](https://www.sciencedirect.com/science/article/pii/S1470160X14001757) by the Ecological Footprint authors
3. A [further comment](https://www.sciencedirect.com/science/article/pii/S1470160X14002726) by Giampietro and Saltelli  
4. A [final paper](https://www.sciencedirect.com/science/article/pii/S1470160X16301844) authored by both teams together with the journal's editor in order to compare the positions.

### Ecological footprint: Analysis/application of the tool

The metric of the Ecological Footprint analysis is based on the calculation of two terms:

1. the ecological footprint itself, i.e. the human appropriation, or demand on the biosphere based on the actual level of resource consumption; and
2. the biocapacity or “biosphere’s regenerative capacity” to supply these resources.  

When the ecological footprint of a system (an economy, a nation, or a region) is larger than its biocapacity, then we have a situation of “unsustainability” or “overshoot”, which is flagged by a deficit in virtual global land.  This deficit occurs when the virtual global land that would be required to meet the system’s demand is larger than the actual supply of “biosphere’s regenerative capacity” (expressed in virtual global land).

An analysis of the details of the protocol reveal that the mechanism by which ‘demand’ is computed is via the square metres of land which are needed to absorb the CO2 emitted, mostly by the energy sector. Following the evolution of the EF over the last 45 years it becomes apparent that, beyond the increased energy consumption, the planet was at a stationary consumption rate – implying that over the past 45 years the carrying capacity of this planet steadily rose, since the increase in the consumption of food and biomass did not cause any harm to the natural capital of our planet.

Thus, the verdict of sensitivity analysis about the EF is that of the hundreds of factors entering the EF protocol, only those entering the calculation of the CO2 absorption by surface area can have impact on the results. In terms of sensitivity auditing the EF clearly violates the first rule, as its complexity is not justified.

This is just one among the many surprises of the EF. Also, a source of perplexity for practitioners is the relation between the square metres of land (a stock), and the CO2 which need to be absorbed per year (a flow). Many practitioners (e.g. beside the authors of this report: [Blomqvist et al., 2013](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001700); [van den Bergh and Verbruggen, 1999](https://www.sciencedirect.com/science/article/pii/S0921800999000324); see also references in [Galli et al. 2016](https://www.sciencedirect.com/science/article/pii/S1470160X16301844(https://www.sciencedirect.com/science/article/pii/S1470160X16301844)) noticed that e.g. a forest only absorb CO2 at the initial stage of development, and goes to equilibrium with respect to CO2 when the canopy is fully developed.

Because the biocapacity term of the EF increases with increased land productivity the EF perceives the intensification in use of pesticides, synthetic fertilizers and GMO-crops as an improvement. As observed by [van den Bergh and Verbruggen](https://www.sciencedirect.com/science/article/pii/S0921800999000324) the capital sin of the Ecological Footprint accounting is to completely ignore the distinction between sustainable and non-sustainable land uses.

Quantitative story telling notes that a number of important threats to sustainability are not included in the EF’s accounting, such as: shortage of water resources, soil health, shortage of minerals, accumulation of nitrogen and phosphorous generating eutrophication in water bodies, accumulation of pollutants in the atmosphere, such as endocrine disruptors, persistent organic pollutants, and pesticides, some of which can undergo biological amplification, threats to biodiversity , and so on. The reader wishing a synthesis of these topics can read a paper co-written with Global Footprint Network at the request of the editor of journal Ecological Indicators, see [Galli et al., 2016](https://www.sciencedirect.com/science/article/pii/S1470160X16301844).  

### Ecological footprint: Key outcomes and lessons learnt

As mentioned above many practitioners have deconstructed over two decades the EF’s attempt to compress the complex pattern of man’s pressure on earth to a single figure related to the number of square metres of land used by man. These deconstructions have not apparently reduced the extraordinary media success of the Ecological Footprint. Not even the Report by the Commission on the Measurement of Economic Performance and Social Progress led by J. Stiglitz, JP Fitoussi and A. Sen [CMEPSP, 2009](http://ec.europa.eu/eurostat/documents/118025/118123/Fitoussi+Commission+report), which gave a negative judgment of the EF in 2009, did change this state of affairs. The invulnerably of the EF is to a large extent due to its complex algorithmic nature. It takes time and effort to understand what it does. This poses an evident ethical dilemma, as one is tempted to balance EF’s faults – evident only to practitioners - with its extraordinary effectiveness in promoting a virtuous narrative, fungible to the many, that man’s use of natural resources is unsustainable, and which allows statements of the sort “If people across the globe had the ecological footprint of those in the United States, we'd need 5 to 7 planets to sustain it all”.

### Ecological footprint: Recommendations

The recommendation from the Siglitz-Fitoussi-Sen [CMEPSP, 2009](http://ec.europa.eu/eurostat/documents/118025/118123/Fitoussi+Commission+report) report is not to use the EF (p. 71):

> “As a result, less-encompassing but more-rigorously-defined footprints, such as the “Carbon Footprint” (CF), would seem better-suited [than the EF], insofar as they are more clearly physical measures of stocks that do not rely on specific assumptions about productivity or an equivalence factor. As far as communications is concerned, such an indicator is just as capable of sending strong messages in terms of the over-utilization of the planet’s capacity for absorption. The CF also has the interesting feature of being computable at any level of disaggregation. This makes it a powerful instrument for monitoring the behaviour of individual actors.”

Beyond the case of the EF proper the lesson of this case is that important normative choices need to be taken in the use of evidence. The field of quantification in particular is undergoing rapid changes, see a review in [Popp Berman and Hirschman](https://osf.io/preprints/socarxiv/4sgc2/), about the nascent ‘Sociology of Quantification’, and ethics is likely to play an increasing role in the evaluation of rankings, indicators, models, and algorithms – terms whose meaning tend to blur in the age of big data.

## Food security: feeding the planet in 2050

### Food security: Introduction to the case

A recently published paper [Badur et al., 2016](https://www.thesolutionsjournal.com/article/pathways-leading-sustainable-healthy-global-food-system/) suggests that improving in agricultural techniques and adopting better dietary styles will lead to producing more food in less land, as to feed, in 2050, ten billion people. The scenario proposed in the paper, which is not uncommon in the present discourse on food security, frames the world as suffering from obesity in the developed countries and hunger in developing countries because of the inappropriateness of the global food production system.

The scenario foresees the adoption of better diets and the contextual reduction of common diseases such as obesity and diabetes which are arrived at thanks to the world agriculture reducing the production of cereals, starches, oils, fats, and sugars in favour of that of fruit and vegetables.

The policy measures advocated to achieve these results include consumer education, better food literacy and cooking skills, taxing unhealthy food, limiting the use of antibiotics and reducing greenhouse gas emission in agriculture, reducing the US corn subsidy, and realizing better storage facilities in developing countries. Note that all measures but the last are to be implemented in developed countries.

We take this scenario as to exemplify techno-optimism in agricultural studies, and in a paper published in Food Ethics [Saltelli and Lo Piano 2017](https://link.springer.com/article/10.1007/s41055-017-0020-6) we apply quantitative storytelling to check the quality of its narrative.  

### Food security: Analysis through the lens/application of the tool

The key points identified by our analysis can be summarized as follows (see [Saltelli and Lo Piano](https://link.springer.com/article/10.1007/s41055-017-0020-6) for the full analysis).

* How can [Badur et al.](https://www.thesolutionsjournal.com/article/pathways-leading-sustainable-healthy-global-food-system/) quantify food production in 2050 at ‘438 million hectares’, with three significant digits? An analysis of the quality of the available data suggests that a one-digit precision would be realistic.

* Following the numbers given by these authors one can note that a 9% reduction in land use, a 1% yearly improvement in production between now and 2050, and a population of 10 billion at that point in time balance out to give in 2050 the same amount of food per capita in 2050 as today. Hence the future scenario does not generate more food per person on average.

* Assuming that agriculture can grow on average by 1% between now and 2050 implying neglecting the existing and projected stress on soils. Precision agriculture might perhaps alleviate the problem.

* Taking the road of low-input agriculture would require more labour to replace the lack of external inputs. This would imply a global agricultural system where the workforce will be devoted to food production with less room for the industrial and service sector, let alone the compression of free time available for leisure activities. Would high-income and mid-high-income countries accept this? 

* Will people desire to adopt a less cereal-and-meat-based diet? In 2050 there will be a higher share of adults given the forecasted reduction in fertility, and adults need more calories than children. Additionally, existing literature points to an increasing consumption of meat in developing countries.

* As per the role of education, [Badur et al.](https://www.thesolutionsjournal.com/article/pathways-leading-sustainable-healthy-global-food-system/) present smoking as an example of how a combination of better policy and education may lead to better habits. In fact, while smoking decreases in developed countries it increases in many developing ones. Developing countries have weak regulatory systems, less capable to counteract food lobbies, so that the desired policies are predicated on a global improvement of governance.

An alternative framing of the issue could instead consider global imbalances and injustices between the global south and the global north. 
Asymmetries in the political power of trade patterns are exactly at the root of the issue of diet quality in several areas of the world, a phenomenon that has been recently named ‘caloric unequal exchange’. Although the export of Latin America and the Caribbean to the rest of the world are more expensive than those imported, the ratio of the two is decreasing with time, with the global south subsidizing the diet of the global north.

Hence the scenario proposed by [Badur et al.](https://www.thesolutionsjournal.com/article/pathways-leading-sustainable-healthy-global-food-system/) applies a developed world perspective, substituting a political problem (power asymmetries) with a technical one (a mismatch between what the world needs for everyone to enjoy a nutritious diet and what the world is actually producing). This approach is perhaps unethical.

A more radical reading of the case, as offered for example by economist [Erik Reinert](https://books.google.com/books?id=GdK3AAAAIAAJ&q=How+rich+countries+got+rich+and+poor&dq=How+rich+countries+got+rich+and+poor&hl=en&sa=X&ved=0ahUKEwj25--e_s7dAhVGFTQIHZb_ANcQ6AEIKTAA) would note that the present trade patterns and economic consensus based of Ricardian economics and free trade – whereby the best option for a country exporting bananas is to stick to its competitive advantage by keeping exporting bananas - are in fact a recipe to keep the poor countries poor. Exchanging raw material against finished goods has never made countries rich.  

### Food security: Key outcomes and lessons learnt

This test cases offer an example of how to identify fragile narratives, i.e. narratives based on a unidirectional vision of the problem is. On this case it was a framing of the problem where the world suffers from conjoint obesity and hunger because of the food production system, and not because of global inequalities between haves and haves not.   As the study rested on dubious hypotheses a combination of sensitivity auditing and quantitative storytelling could identify the weaknesses - both technical and normative - of the study.

### Recommendations

Reflecting on the framing of an assessment is precious in terms of anticipating criticism, and is hence relevant when analyzing, e.g. in the field of sustainability, different scenarios, trajectories for transitions , outlooks, distance-to-target and so on. It is advisable to identify the assumptions underlying e.g. a suggested transition to a lower consumption pattern in the terms suggested by quantitative storytelling.

## Other applications

Other applications which can be referred to sensitivity auditing and quantitative storytelling are to the OECD-PISA study [Araújo et al., 2017](https://www.emeraldinsight.com/doi/abs/10.1108/IJCED-12-2016-0023), [Saltelli, 2017](https://theconversation.com/international-pisa-tests-show-how-evidence-based-policy-can-go-wrong-77847), and to the controversy surrouding the use of Golden Rice, a GMO crop [Saltelli et al, 2017](https://theconversation.com/forcing-consensus-is-bad-for-science-and-society-77079).
